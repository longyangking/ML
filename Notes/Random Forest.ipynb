{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Decision Tree\n",
    "+ Classification Tree\n",
    "+ Regression Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Learing Algorithms\n",
    "+ ID3 (Iterative Dichotomiser 3)\n",
    "+ C4.5 (Successor of ID3)\n",
    "+ CART (Classification and regression tree)\n",
    "+ CHAID (CHi-squared automatic interaction detector)\n",
    "+ MARS\n",
    "+ Conditional Inference Trees (Statistics-based approach)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterative Dichotomiser 3 (ID3)\n",
    "In decision tree learning, ID3 is an algorithm invented by Ross Quinlan used to generate a decision tree from a dataset. ID3 is the precursor to the C4.5 algorithm, and is typically used in the machine learning and natural language processing domains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pseudocode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ID3 (Examples, Target_Attribute, Attributes)\n",
    "    Create a root node for the tree\n",
    "    If all examples are positive, Return the single-node tree Root, with label = +.\n",
    "    If all examples are negative, Return the single-node tree Root, with label = -.\n",
    "    If number of predicting attributes is empty, then Return the single node tree Root,\n",
    "    with label = most common value of the target attribute in the examples.\n",
    "    Otherwise Begin\n",
    "        A ← The Attribute that best classifies examples.\n",
    "        Decision Tree attribute for Root = A.\n",
    "        For each possible value, vi, of A,\n",
    "            Add a new tree branch below Root, corresponding to the test A = vi.\n",
    "            Let Examples(vi) be the subset of examples that have the value vi for A\n",
    "            If Examples(vi) is empty\n",
    "                Then below this new branch add a leaf node with label = most common target value in the examples\n",
    "            Else below this new branch add the subtree ID3 (Examples(vi), Target_Attribute, Attributes – {A})\n",
    "    End\n",
    "    Return Root"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entropy\n",
    "Entropy $H(S)$ is a measure of the amount of uncertainty in the data set $S$ (i.e. entropy characterizes the data set $S$).\n",
    "$$H(S) = - \\sum_{x\\in X} p(x) log_2 p(x)$$\n",
    "\n",
    "where,\n",
    "+ $S$ - The current data set for which entropy is being calculated (changes every iteration of the ID3 algorithm)\n",
    "+ $X$ - Set of classes in $S$\n",
    "+ $p(x)$ - The proportion of the number of elements in class $x$ to the number of elements in set $S$\n",
    "When $H(S)=0$, the set $S$ is perfectly classified (i.e. all elements in $S$ are of the same class).\n",
    "\n",
    "In ID3, entropy is calculated for each remaining attribute. The attribute with the __smallest__ entropy is used to split the set $S$ on this iteration. The higher the entropy, the higher the potential to improve the classification here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Information gain\n",
    "Information gain $IG(A)$ is the measure of the difference in entropy from before to after the set $S$ is split on an attribute $A$. In other words, how much uncertainty in $S$ was reduced after splitting set $S$ on attribute $A$.\n",
    "$$IG(A,S) = H(S) - \\sum_{t\\in T} p(t)H(t)$$\n",
    "\n",
    "where,\n",
    "+ $H(S)$ - Entropy of set $S$\n",
    "+ $T$ - The subsets created from splitting set $S$ by attribute $A$ such that $S = \\bigcup_{t\\in T} t$\n",
    "+ $p(t)$ - The proportion of the number of elements in $t$ to the number of elements in set $S$\n",
    "+ $H(t)$ - Entropy of subset $t$\n",
    "\n",
    "In ID3, information gain can be calculated(instead of entropy) for each remaining attribute. The attribute with the __largest__ information gain is used to split the set $S$ on this iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "+ Gini impurity\n",
    "+ Information gain\n",
    "+ Variance reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "+ Bagging\n",
    "+ Boosted trees\n",
    "+ Rotation forest"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
