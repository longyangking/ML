\documentclass[10pt,a4paper,draft]{book}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{bm}

\author{Yang Long}
\title{Chapter 1 - Solutions}

\begin{document}
\maketitle
\section*{1.1}
Consider polynomial function of the form $y(x,\bm{w})$ as:
\begin{equation}
y(x_n,\bm{w}) = \sum_{j=0}^M \omega_j x_n^j
\end{equation}
To minimize the \emph{error function} $E(\bm{w})$  with the corresponding target values $t_n$:
\begin{equation}
E(\bm{w}) = \frac{1}{2}\sum_{n=1}^N \{y(x_n,\bm{w})-t_n\}^2
\end{equation}
Following the set of equations:
\begin{equation}
\sum_{j=0}^M A_{ij}\omega_j  = T_i, \quad
A_{ij} = \sum_{n=1}^{N} (x_n)^{i+j}, \quad T_i = \sum_{n=1}^{N} (x_n)^i t_n
\end{equation}
\begin{equation}
\sum_{j=0}^M \sum_{n=1}^{N} (x_n)^{i+j}\omega_j  = \sum_{n=1}^{N} (x_n)^i t_n, \Rightarrow \sum_{n=1}^{N} (x_n)^i\left(\sum_{j=0}^M (x_n)^{j}\omega_j - t_n\right) = 0
\end{equation}
\begin{equation}
\sum_{n=1}^{N} (x_n)^i\left(y(x_n,\bm{w}) - t_n\right) = 0, \forall i 
\end{equation} 
To show that the coefficients $\bm{w} = \{\omega_i\}$ that can minimize the error function, so that:
\begin{equation}
\frac{\partial E(\bm{w})}{\partial \omega_i} = 0
\end{equation}
\begin{equation}
\begin{aligned}
\frac{\partial E(\bm{w})}{\partial \omega_i} &=\sum_{n=0}^N \{y(x_n,\bm{w})-t_n\} \frac{\partial y(x_n,\bm{w})}{\partial \omega_i} \\
&=\sum_{n=0}^N \{y(x_n,\bm{w})-t_n\} x_n^i \\
&=0
\end{aligned}
\end{equation}
Thus, the coefficients $\bm{w} = \{\omega_i\}$ can minimize the error function $E(\bm{w})$.

\section*{1.2}
The regularized sum-of-squares error function (1.4) in the main text is:
\begin{equation}
\tilde{E}(\bm{w}) = \frac{1}{2}\sum_{n=0}^N\{y(x_n,\bm{w}) - t_n\}^2 + \frac{\lambda}{2} ||\bm{w}||^2
\end{equation}
where $||\bm{w}||^2 = \sum_{i=0}^M \omega_i^2$. To minimize the error function, one should set:
\begin{equation}
\frac{\partial \tilde{E}(\bm{w})}{\partial \omega_i} =\sum_{n=0}^N \{y(x_n,\bm{w})-t_n\} \frac{\partial y(x_n,\bm{w})}{\partial \omega_i} +\lambda \omega_i=  0
\end{equation}
\begin{equation}
\sum_{n=0}^N \{y(x_n,\bm{w})-t_n\} x_n^i +\lambda \omega_i =0 
\end{equation}
\begin{equation}
\sum_{n=1}^{N} (x_n)^i\left(\sum_{j=0}^M (x_n)^{j}\omega_j - t_n\right) + \lambda \omega_i =0
\end{equation}
\begin{equation}
\sum_{j=0}^M \sum_{n=1}^{N} (x_n)^{i+j}\omega_j + \lambda \omega_i = \sum_{j=0}^M \left(\sum_{n=1}^{N} (x_n)^{i+j} + \lambda \delta_{ij}\right)\omega_j = \sum_{n=1}^{N} (x_n)^i t_n
\end{equation}
\begin{equation}
\sum_{j=0}^M A_{ij}\omega_j= T_i, \quad A_{ij} = \sum_{n=1}^{N} (x_n)^{i+j} + \lambda \delta_{ij} 
\end{equation}
where delta function $ \delta_{ij}=\left\{
\begin{aligned}
1,\quad & i = j \\
0,\quad & i \neq j
\end{aligned}
\right.$.

\section*{1.3}
According to the settings, we have
\begin{equation}
\begin{aligned}
p(r) &= 0.2, \quad p(b) = 0.2, \quad p(g) = 0.6 \\
p(apple|r) &= 0.3, \quad p(apple|b) = 0.5,\quad p(apple|g) = 0.3\\
p(orange|r) &= 0.4, \quad p(orange|b) = 0.5,\quad p(orange|g) = 0.3\\
p(lime|r) &= 0.3, \quad p(lime|b) = 0,\quad p(lime|g) = 0.4
\end{aligned}
\end{equation}
The the probability of selecting an apple:
\begin{equation}
p(apple) = p(apple|r)p(r) + p(apple|b)p(b) + p(apple|g)p(g) = 0.34
\end{equation}
If selected fruit is an orange, the probability that it came from the green box:
\begin{equation}
p(g|apple) = \frac{p(apple|g)p(g)}{p(apple)} = 0.53
\end{equation}

\section*{1.4}
In the case of a linear transformation, i.e, $x = g(y) = k y + c$, so that 
\begin{equation}
\begin{aligned}
p_y(y) &= p_x(x) \left|\frac{dx}{dy}\right| = p_x(g(y))|g'(y)| \\
&= k p_x(x)
\end{aligned}
\end{equation}
\begin{equation}
\begin{aligned}
\frac{\partial p_y(y)}{\partial y}&= k \frac{\partial p_x(x)}{\partial y}
&= k^2 \frac{\partial p_x(x)}{\partial x}
\end{aligned}
\end{equation}
Because $\frac{\partial p_x(\hat{x})}{\partial x} = 0$, $\frac{\partial p_y(\hat{y})}{\partial y} = 0$, where $\hat{x} = k \hat{y} + c$. 

Thus, in the case of a linear transformation, the location of maximum transforms in the same way as the variable itself. But it will be totally different for non-linear transformation. 
\begin{equation}
\begin{aligned}
\frac{\partial p_y(y)}{\partial y}&= \frac{\partial p_x(x)|g'(y)|}{\partial y}
=s (g'(y))^2 \frac{\partial p_x(x) }{\partial x} + s p_x(x) g''(y)
\end{aligned}
\end{equation}
wher $s \in \{-1,+1\}$. Although $\frac{\partial p_x(\hat{x})}{\partial x} = 0$, it can not be satisfied that $p_x(\hat{x}) g''(\hat{y})=0$, namely, $\frac{\partial p_y(\hat{y})}{\partial y} = 0$. For general transformation, the maximum of probability after transformation can not be guaranteed, which will be dependent on the choice of variable.

\section*{1.5}
The definition of variance of $f(x)$ is:
\begin{equation}
\begin{aligned}
var[f] &= \mathbb{E}[(f(x)-\mathbb{E}[f(x)])^2] \\
&= \mathbb{E}[f(x)^2-2\mathbb{E}[f(x)]f(x)+ \mathbb{E}[f(x)]^2] \\
&= \mathbb{E}[f(x)^2]-2\mathbb{E}[f(x)]^2 + \mathbb{E}[f(x)]^2] \\
&= \mathbb{E}[f(x)^2]-\mathbb{E}[f(x)]^2
\end{aligned}
\end{equation}
QED

\section*{1.6}
If $x$ and $y$ are independent, then $p(x,y) = p(x)p(y)$. According to the definition of covariance, one have:
\begin{equation}
\begin{aligned}
cov[x,y] &= \mathbb{E}_{x,y}[(x-\mathbb{E}[x])(y-\mathbb{E}[y])] \\
&= \mathbb{E}_{x,y}[xy - \mathbb{E}[x]y - \mathbb{E}[y]x + \mathbb{E}[x]\mathbb{E}[y]] \\
&= \mathbb{E}_{x,y}[xy] - \mathbb{E}[x]\mathbb{E}_{x,y}[y] - \mathbb{E}[y]\mathbb{E}_{x,y}[x] + \mathbb{E}[x]\mathbb{E}[y] \\
&= \int p(x,y)xydxdy - \int p(x)xdx \int p(x,y)ydxdy \\ &- \int p(y)ydy \int p(x,y)xdxdy + \int p(x)xdx \int p(y)ydy \\
&= \int p(x)p(y)xydxdy - \int p(x)xdx \int p(x)p(y)ydxdy \\ &- \int p(y)ydy \int p(x)p(y)xdxdy + \int p(x)xdx \int p(y)ydy \\
&= \mathbb{E}[x]\mathbb{E}[y] - \mathbb{E}[x]\mathbb{E}[y] -\mathbb{E}[x]\mathbb{E}[y] + \mathbb{E}[x]\mathbb{E}[y] \\
&= 0
\end{aligned}
\end{equation}

\section*{1.7}
Consider the transformation from Cartesian coordinates $(x,y)$ to polar coordinates $(r,\theta)$, 
\begin{equation}
x = r cos(\theta), \quad y = r sin(\theta)
\end{equation}
\begin{equation}
\begin{aligned}
\begin{pmatrix}
dx \\
dy
\end{pmatrix} &=
\frac{\partial(x,y)}{\partial(r,\theta)}
\begin{pmatrix}
dr \\
d\theta
\end{pmatrix} = 
\begin{pmatrix}
\frac{\partial x}{\partial r} & \frac{\partial x}{\partial \theta} \\
\frac{\partial y}{\partial r} & \frac{\partial y}{\partial \theta} 
\end{pmatrix} 
\begin{pmatrix}
dr \\
d\theta
\end{pmatrix} \\
&=
\begin{pmatrix}
cos(\theta) & -r sin(\theta) \\
sin(\theta) & r cos(\theta) 
\end{pmatrix} 
\begin{pmatrix}
dr \\
d\theta
\end{pmatrix}
\end{aligned}
\end{equation}
Thus the integral can be rewritten as:
\begin{equation}
\begin{aligned}
I^2 &=  \int_0^{2\pi} \int_0^{\infty} exp(-\frac{r^2}{2\sigma^2}r dr d\theta \\
&= 2\pi\int_0^{\infty} \frac{1}{2}exp(-\frac{u}{2\sigma^2})du \\
&= 2\pi\sigma^2
\end{aligned}
\end{equation}
where $u=r^2$. So we have $I = \sqrt{2\pi\sigma^2}$.

For univariate Gaussian distribution, $p(x) ~ \mathcal{N}(x|\mu,\sigma^2)$, by using transformation $y = x - \mu$, one can get the integral of Gaussian distribution as:
\begin{equation}
\begin{aligned}
\int_{-\infty}^{\infty}\mathcal{N}(x|\mu,\sigma^2) dx &= \frac{1}{\sqrt{2\pi\sigma^2}} \int_{-\infty}^{\infty} exp(-\frac{y^2}{2\sigma^2}) dy \\
&= \frac{I}{\sqrt{2\pi\sigma^2}} = 1
\end{aligned}
\end{equation}
QED.

\section*{1.8}
According to the Eq.(1.46):
\begin{equation}
\begin{aligned}
\mathbb{E}[x] &= \int_{-\infty}^{\infty}\mathcal{N}(x|\mu,\sigma^2)x dx \\
&= \int_{-\infty}^{\infty}\mathcal{N}(y|0,\sigma^2)(y + \mu) dy \\
&= \int_{-\infty}^{\infty}\mathcal{N}(y|0,\sigma^2)ydy + \mu\int_{-\infty}^{\infty}\mathcal{N}(y|0,\sigma^2)dy
\end{aligned}
\end{equation}
where $y = x-\mu$.Based on the fact:
\begin{equation}
\int_{-\infty}^{\infty}\mathcal{N}(y|0,\sigma^2)ydy = 0, \quad \int_{-\infty}^{\infty}\mathcal{N}(y|0,\sigma^2)dy = 1
\end{equation}
Thus, $\mathbb{E}[x] = \mu$, QED in (1.49).

By differentiating both sides of Eq.(1.127) with respect to $\sigma^2$, one have:
\begin{equation}
\begin{aligned}
\frac{\partial}{\partial (\sigma^2)} \left(\int_{-\infty}^{\infty}\mathcal{N}(x|\mu,\sigma^2) dx\right) &= 0 \\
\int_{-\infty}^{\infty} \frac{\partial}{\partial (\sigma^2)} \mathcal{N}(x|\mu,\sigma^2) dx &= 0
\end{aligned}
\end{equation}
\begin{equation}
\begin{aligned}
\frac{\partial}{\partial (\sigma^2)} \mathcal{N}(x|\mu,\sigma^2) &= \frac{(x-\mu)^2 - \sigma^2}{2\sigma^4} \mathcal{N}(x|\mu,\sigma^2) = 
\frac{x^2 - 2x\mu+\mu^2 - \sigma^2}{2\sigma^4} \mathcal{N}(x|\mu,\sigma^2)
\end{aligned}
\end{equation}
\begin{equation}
\begin{aligned}
\int_{-\infty}^{\infty} \frac{\partial}{\partial (\sigma^2)} \mathcal{N}(x|\mu,\sigma^2) dx &= \int_{-\infty}^{\infty}(x^2 - 2x\mu+\mu^2 - \sigma^2)\mathcal{N}(x|\mu,\sigma^2) \\
& = \int_{-\infty}^{\infty}x^2\mathcal{N}(x|\mu,\sigma^2) - \int_{-\infty}^{\infty}(2x\mu-\mu^2+\sigma^2)\mathcal{N}(x|\mu,\sigma^2) \\
&= \int_{-\infty}^{\infty}x^2\mathcal{N}(x|\mu,\sigma^2) - (\mu^2+\sigma^2) = 0
\end{aligned}
\end{equation}
\begin{equation}
\begin{aligned}
\mathbb{E}[x^2] = \int_{-\infty}^{\infty}x^2\mathcal{N}(x|\mu,\sigma^2) = \mu^2+\sigma^2
\end{aligned}
\end{equation}
QED in (1.50)

It's clear that:
\begin{equation}
var[x] = \mathbb{E}[x^2] - \mathbb{E}[x]^2 = \mu^2 + \sigma^2 - \mu^2 = \sigma^2
\end{equation}
QED in (1.51)

\section*{1.9}
At the mode of the Gaussian distribution, one have
\begin{equation}
\begin{aligned}
\left[\frac{\partial}{\partial x} \mathcal{N}(x|\mu,\sigma^2) \right]_{x=\hat{x}}= -\left[\frac{x-\mu}{\sigma^2} \mathcal{N}(x|\mu,\sigma^2) \right]_{x=\hat{x}}= 0
\end{aligned}
\end{equation}
Thus, $\hat{x} = \mu$.

Similarly, at the mode of the multivariate Gaussian, one have
\begin{equation}
\begin{aligned}
\frac{\partial}{\partial \bm{x}} \mathcal{N}(\bm{x}|\bm{\mu},\bm{\Sigma}) &= -\frac{1}{2}\mathcal{N}(\bm{x}|\bm{\mu},\bm{\Sigma}) \frac{\partial}{\partial \bm{x}} ((\bm{x}-\bm{\mu})^T\bm{\Sigma}^{-1}(\bm{x}-\bm{\mu}))  \\
&= -\mathcal{N}(\bm{x}|\bm{\mu},\bm{\Sigma}) \bm{\Sigma}^{-1}(\bm{x}-\bm{\mu}) \\
&= 0
\end{aligned}
\end{equation}
Based on $\left[\frac{\partial}{\partial \bm{x}} \mathcal{N}(\bm{x}|\bm{\mu},\bm{\Sigma})\right]_{\bm{x} = \hat{\bm{x}}} = 0$, thus, $\hat{\bm{x}} = \bm{\mu}$. QED

\section*{1.10}
Suppose that the two variables $x$ and $z$ are statistically independent, namely, $p(x,z) = p(x)p(z)$, so that:
\begin{equation}
\begin{aligned}
\mathbb{E}[x+z] &= \int (x+z)p(x)p(y) dxdz \\
&= \int x p(x)p(z) dxdz + \int z p(x)p(z) dxdz \\
&= \mathbb{E}[x] + \mathbb{E}[z] 
\end{aligned}
\end{equation}
\begin{equation}
\begin{aligned}
var[x+z] &= \mathbb{E}[(x+z - \mathbb{E}[x+z])^2] \\ 
&= \mathbb{E}[(x+z)^2 - 2(x+z)\mathbb{E}[x+z] + \mathbb{E}[x+z]^2] \\
&= \mathbb{E}[(x+z)^2] - \mathbb{E}[x+z]^2 \\
&= \mathbb{E}[x^2+2xz+z^2] - (\mathbb{E}[x] + \mathbb{E}[z] )^2 \\
&= \mathbb{E}[x^2]+\mathbb{E}[2xz]+\mathbb{E}[z^2] - \mathbb{E}[x]^2 - 2\mathbb{E}[x]\mathbb{E}[z]- \mathbb{E}[z]^2 \\
&= \mathbb{E}[x^2]- \mathbb{E}[x]^2+\mathbb{E}[z^2]- \mathbb{E}[z]^2\\
&=var[x]+var[z]
\end{aligned}
\end{equation}
QED

\section*{1.11}
By setting the derivatives of the log likelihood function with respect to $\mu$ and $\sigma^2$ equal to zero:
\begin{equation}
\begin{aligned}
\frac{\partial}{\partial \mu} ln p(\bm{x}|\mu,\sigma^2) & = \frac{1}{\sigma^2}\sum_{n=1}^{N} (x_n-\mu) \\
&=  \frac{1}{\sigma^2}\sum_{n=1}^{N} x_n-\frac{1}{\sigma^2}\sum_{n=1}^{N}\mu \\
&= 0
\end{aligned}
\end{equation}
Thus:
\begin{equation}
\mu_{ML} = \frac{1}{N}\sum_{n=1}^{N} x_n
\end{equation}
In the same way,
\begin{equation}
\begin{aligned}
\frac{\partial}{\partial \sigma^2} ln p(\bm{x}|\mu,\sigma^2) & = \frac{1}{2\sigma^4}\sum_{n=1}^{N} (x_n-\mu)^2 - \frac{N}{2\sigma^2} = 0
\end{aligned}
\end{equation}
Thus:
\begin{equation}
\sigma^2_{ML} = \frac{1}{N}\sum_{n=1}^{N} (x_n-\mu_{ML})^2
\end{equation}
QED

\section*{1.12}
If $n=m$, then $\mathbb{E}[x_n^2] = \mu^2 + \sigma^2$, if $n\neq m$, then $\mathbb{E}[x_nx_m] = \mathbb{E}[x_n]\mathbb{E}[x_m] = \mu^2$.
\begin{equation}
\begin{aligned}
\mathbb{E}[\mu_{ML}] = \frac{1}{N}\sum_{n=1}^{N}\mathbb{E}[x_n] = \mu
\end{aligned}
\end{equation}
\begin{equation}
\begin{aligned}
\mathbb{E}[\sigma^2_{ML}] &= \frac{1}{N}\sum_{n=1}^{N}\mathbb{E}[(x_n-\mu_{ML})^2] \\
&= \frac{1}{N}\sum_{n=1}^{N}(\mathbb{E}[x_n^2] - 2 \mathbb{E}[x_n\mu_{ML}]+\mathbb{E}[\mu_{ML}^2])
\end{aligned}
\end{equation}
\begin{equation}
\begin{aligned}
\mathbb{E}[x_n^2] &= \mu^2 + \sigma^2 \\
\mathbb{E}[x_n\mu_{ML}] &= \frac{1}{N}\mathbb{E}[\sum_{m=1}^{N}x_nx_m] = \mu^2 + \frac{1}{N}\sum_{m=1}^{N} I_{mn}\sigma^2 \\
\mathbb{E}[\mu_{ML}^2] &= \frac{1}{N^2}\sum_{m=1}^{N}\sum_{i=1}^{N} \mathbb{E}[x_mx_i] = \mu^2 + \frac{1}{N}\sigma^2
\end{aligned}
\end{equation}
Thus,
\begin{equation}
\begin{aligned}
\mathbb{E}[\sigma^2_{ML}] &= \frac{1}{N}\sum_{n=1}^{N}(\mathbb{E}[x_n^2] - 2 \mathbb{E}[x_n\mu_{ML}]+\mathbb{E}[\mu_{ML}^2]) \\
&= \mu^2+\sigma^2 - 2(\mu^2 + \frac{1}{N}\sigma^2) + \mu^2 + \frac{1}{N}\sigma^2 \\
&= (1-\frac{1}{N})\sigma^2 = \frac{N-1}{N}\sigma^2
\end{aligned}
\end{equation}
QED

\section*{1.13}


\end{document}
